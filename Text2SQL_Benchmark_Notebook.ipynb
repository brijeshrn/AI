{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef1c0e5",
   "metadata": {},
   "source": [
    "# Benchmarking Text-to-SQL: Evaluating AI on Complex Natural Language to SQL Tasks\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** [Today's Date]\n",
    "\n",
    "## Why this Notebook?\n",
    "\n",
    "As Natural Language Processing (NLP) models become more powerful, the ambition to query databases using plain English grows. But bridging the gap between human queries and database logic isn't trivial—especially for real-world, complex business questions.\n",
    "\n",
    "This notebook walks you through:\n",
    "- Why Text-to-SQL is challenging\n",
    "- How to build your own benchmark set of complex NLQ (Natural Language Queries) and SQL pairs\n",
    "- How to set up, document, and share your evaluation process\n",
    "\n",
    "*Let’s dive in!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d87fee",
   "metadata": {},
   "source": [
    "## Why Focus on Complex Queries?\n",
    "\n",
    "Most simple queries (like \"Show all customers\") don’t test the true capabilities of a Text-to-SQL system. In real business settings, users ask questions involving:\n",
    "\n",
    "- **Aggregations:** Summaries like averages, counts, sums\n",
    "- **Joins:** Merging information across tables\n",
    "- **Filtering and Grouping:** Extracting patterns and segments\n",
    "- **Orderings and Limits:** Prioritizing the most relevant results\n",
    "\n",
    "These are exactly where most models struggle—and where your benchmark needs to shine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a list of complex NLQ (Natural Language Query) and Gold SQL pairs\n",
    "complex_samples = [\n",
    "    {\n",
    "        \"NLQ\": \"What is the full name and id of the college with the largest number of baseball players?\",\n",
    "        \"Gold SQL\": \"\"\"\n",
    "        SELECT T1.name_full, T1.college_id\n",
    "        FROM college AS T1\n",
    "        JOIN player_college AS T2 ON T1.college_id = T2.college_id\n",
    "        GROUP BY T1.college_id\n",
    "        ORDER BY COUNT(*) DESC\n",
    "        LIMIT 1;\n",
    "        \"\"\",\n",
    "        \"Complexity\": \"Aggregation, Join, Group By, Ordering, Limit\",\n",
    "        \"Why_Complex\": \"This query combines a join between two tables, groups the result, counts players per college, orders by that count, and limits to the top college. It reflects business questions like 'Who is our top customer segment?'\"\n",
    "    },\n",
    "    {\n",
    "        \"NLQ\": \"What is average salary of the players in the team named 'Boston Red Stockings'?\",\n",
    "        \"Gold SQL\": \"\"\"\n",
    "        SELECT AVG(T1.salary)\n",
    "        FROM salary AS T1\n",
    "        JOIN team AS T2 ON T1.team_id = T2.team_id_br\n",
    "        WHERE T2.name = 'Boston Red Stockings';\n",
    "        \"\"\",\n",
    "        \"Complexity\": \"Join, Aggregation, Filtering\",\n",
    "        \"Why_Complex\": \"Requires finding the correct team by name, joining salary and team tables, and computing an average. It mimics executive analytics like 'What’s the average cost per department?'\"\n",
    "    },\n",
    "    {\n",
    "        \"NLQ\": \"What are first and last names of players participating in all star game in 1998?\",\n",
    "        \"Gold SQL\": \"\"\"\n",
    "        SELECT name_first, name_last\n",
    "        FROM player AS T1\n",
    "        JOIN all_star AS T2 ON T1.player_id = T2.player_id\n",
    "        WHERE YEAR = 1998;\n",
    "        \"\"\",\n",
    "        \"Complexity\": \"Join, Filtering\",\n",
    "        \"Why_Complex\": \"Links two tables on player ID and filters by year—a step up from just querying one table.\"\n",
    "    },\n",
    "    {\n",
    "        \"NLQ\": \"What are the first name, last name and id of the player with the most all star game experiences?\",\n",
    "        \"Gold SQL\": \"\"\"\n",
    "        SELECT name_first, name_last, player_id\n",
    "        FROM player AS T1\n",
    "        JOIN all_star AS T2 ON T1.player_id = T2.player_id\n",
    "        GROUP BY T1.player_id\n",
    "        ORDER BY COUNT(*) DESC\n",
    "        LIMIT 1;\n",
    "        \"\"\",\n",
    "        \"Complexity\": \"Join, Aggregation, Group By, Ordering, Limit\",\n",
    "        \"Why_Complex\": \"Finds the player with the maximum number of all-star games—a multi-step analytic involving joins, grouping, and aggregation.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(complex_samples)\n",
    "df[['NLQ', 'Complexity', 'Why_Complex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e82c51",
   "metadata": {},
   "source": [
    "## Visualizing Our Evaluation Set\n",
    "\n",
    "Below, we display the NLQs, what makes them complex, and which SQL features are exercised.  \n",
    "*You can expand this table as your business use cases evolve.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the benchmark set with explanations\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df[['NLQ', 'Complexity', 'Why_Complex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50b4f9",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "\n",
    "- **Expand the Set:** Add your own business-critical NLQ-SQL pairs.\n",
    "- **Connect Your Model:** Use your favorite Text-to-SQL model to generate SQL for each NLQ.\n",
    "- **Evaluate:** Compare model output to the 'Gold SQL' using:\n",
    "  - Exact match (for structure)\n",
    "  - Execution accuracy (do the results match?)\n",
    "  - Partial credit (for close but not exact answers)\n",
    "\n",
    "**Why this matters:**  \n",
    "Domain-relevant queries expose gaps that public datasets may miss, especially in regulated, enterprise, or niche sectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04336d2d",
   "metadata": {},
   "source": [
    "## Model Evaluation (To Be Implemented)\n",
    "\n",
    "Here’s how you could extend this notebook to:\n",
    "- Generate SQL queries using your LLM or Text-to-SQL system\n",
    "- Automatically compare generated SQL with gold SQL\n",
    "- Compute accuracy metrics (exact match, execution match, etc.)\n",
    "\n",
    "*Code for evaluation can be added here in future versions!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15962db7",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Building a benchmark set for Text-to-SQL is an **iterative, ongoing process**.  \n",
    "As your business grows or your schema evolves, revisit your test queries.  \n",
    "\n",
    "**Share this notebook:**  \n",
    "- On GitHub (with your additions)\n",
    "- With your team to ensure everyone speaks the same 'language' when it comes to analytics\n",
    "\n",
    "---\n",
    "\n",
    "**Inspired by hands-on AI work in real enterprises and co-authored with ChatGPT by OpenAI.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
